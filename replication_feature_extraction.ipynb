{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "from functools import reduce\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pathlib as path\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.spatial import KDTree\n",
    "from collections import defaultdict\n",
    "from argparse import Namespace\n",
    "from scipy.spatial.qhull import QhullError\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import h5py as h5\n",
    "from skimage.morphology import watershed, dilation, ball, closing, opening\n",
    "from skimage.external.tifffile import imread\n",
    "from skimage.measure import regionprops\n",
    "from scipy import ndimage as ndi\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minimal size of all files\n",
    "metadata_files = glob.glob('C:/Users/david/Desktop/replication-data/fixed_single_channel2/metadata*.json')\n",
    "\n",
    "sizes = []\n",
    "for file in metadata_files:\n",
    "    with open(file, 'r') as fd:\n",
    "        meta = json.load(fd)\n",
    "    sizes.append([meta['voxel_size_z'], meta['voxel_size_y'], meta['voxel_size_x']])\n",
    "    \n",
    "min_size = np.min(np.array(sizes), axis=0)\n",
    "min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation\n",
    "\n",
    "replication_overlap_thresh = 0.5 # how much of a replication site has to be inside the nuclear mask to keep it\n",
    "\n",
    "def segment(marker_file, plot=False):\n",
    "\n",
    "    # get segmentation/raw/metadata files corresponding to marker file\n",
    "    prob_file = marker_file.replace('markers', 'classification_dapi')\n",
    "    prob_file_edu = marker_file.replace('markers', 'classification_edu').replace('dapi', 'edu')\n",
    "    prob_file_pcna = marker_file.replace('markers', 'classification_pcna').replace('dapi', 'pcna')\n",
    "    \n",
    "    raw_file = marker_file.replace('markers', '').replace('.h5', '.tif')\n",
    "    raw_file_edu = marker_file.replace('markers', '').replace('.h5', '.tif').replace('dapi', 'edu')\n",
    "    raw_file_pcna = marker_file.replace('markers', '').replace('.h5', '.tif').replace('dapi', 'pcna')\n",
    "    \n",
    "    metadata_file = marker_file.replace('markers', '').replace('.h5', '.json').replace('dapi', 'metadata')\n",
    "    \n",
    "    # get zoom factor necessary to have maximal isotropic resolution\n",
    "    with open(metadata_file, 'r') as fd:\n",
    "        meta = json.load(fd)\n",
    "    size = np.array([meta['voxel_size_z'], meta['voxel_size_y'], meta['voxel_size_x']])\n",
    "    zoom_factor = size/np.min(min_size)\n",
    "\n",
    "\n",
    "    with h5.File(marker_file, 'r') as fd:\n",
    "        markers = fd['markers'][...].squeeze()\n",
    "\n",
    "    # read prob files\n",
    "    with h5.File(prob_file, 'r') as fd:\n",
    "        probs = fd['probs'][..., 1] # NB.: [...] converts to np-array\n",
    "        probs_bg = fd['probs'][..., 0]\n",
    "    with h5.File(prob_file_edu, 'r') as fd:\n",
    "        probs_edu = fd['probs'][..., 1]        \n",
    "    with h5.File(prob_file_pcna, 'r') as fd:\n",
    "        probs_pcna = fd['probs'][..., 1]\n",
    "        \n",
    "    # read raw files\n",
    "    raw = imread(raw_file)\n",
    "    raw_edu = imread(raw_file_edu)\n",
    "    raw_pcna = imread(raw_file_pcna)\n",
    "\n",
    "    # introduce background marker 1: areas with high bg prob\n",
    "    # all other labels += 1\n",
    "    markers = ndi.label(markers)[0]\n",
    "    markers[markers>0] += 1\n",
    "    markers[probs_bg > 0.99] = 1 # TODO: make parameter?\n",
    "\n",
    "    # watershed from markers -> dapi segmentation\n",
    "    segmentation = watershed(probs, markers) #, compactness=0.0002) # TODO: compactness necessary?\n",
    "    segmentation[segmentation==1] = 0\n",
    "    \n",
    "    # get the most central object\n",
    "    center = np.array(segmentation.shape).astype(np.float)/2\n",
    "    center_label = None\n",
    "    center_dist = None\n",
    "    for rprop in regionprops(segmentation):\n",
    "        if center_label is None or np.linalg.norm(center-rprop.centroid) < center_dist:\n",
    "            center_label = rprop.label\n",
    "            center_dist = np.linalg.norm(center-rprop.centroid)\n",
    "\n",
    "    seg_dapi = segmentation==center_label\n",
    "                       \n",
    "    # edu/pcna mask -> foreground prob higher than 0.5 \n",
    "    seg_edu = (probs_edu > 0.5).astype(np.int)\n",
    "    seg_pcna = (probs_pcna > 0.5).astype(np.int)\n",
    "    \n",
    "    # watershed on edt to separate replication sites\n",
    "    #seg_edu *= watershed(-ndi.morphology.distance_transform_edt(seg_edu))\n",
    "    seg_edu *= watershed(-ndi.gaussian_filter(ndi.morphology.distance_transform_edt(seg_edu), (0.5,1,1)))\n",
    "    #seg_pcna *= watershed(-ndi.morphology.distance_transform_edt(seg_pcna))\n",
    "    seg_pcna *= watershed(-ndi.gaussian_filter(ndi.morphology.distance_transform_edt(seg_pcna), (0.5,1,1)))\n",
    "    \n",
    "    # remove replication objects not within nuclear mask\n",
    "    for rprop in regionprops(seg_edu):\n",
    "        if np.sum(rprop.image * seg_dapi[rprop.slice]) < replication_overlap_thresh * np.sum(rprop.image):\n",
    "            seg_edu[rprop.slice] = 0    \n",
    "    for rprop in regionprops(seg_pcna):\n",
    "        if np.sum(rprop.image * seg_dapi[rprop.slice]) < replication_overlap_thresh * np.sum(rprop.image):\n",
    "            seg_pcna[rprop.slice] = 0\n",
    "    \n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(ncols=3, figsize=(15,5))\n",
    "        axs[0].imshow(np.max(raw, axis=0), cmap='gray')\n",
    "        axs[1].imshow(np.max(markers, axis=0), cmap='jet', interpolation='nearest')\n",
    "        axs[2].imshow(np.max(seg_dapi, axis=0), cmap='gray', interpolation='nearest')\n",
    "        [ax.axis('off') for ax in axs]\n",
    "        [ax.set_title(t) for t,ax in zip(['DAPI', 'Markers', 'Central nucleus'], axs)]\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        fig, axs = plt.subplots(ncols=2, figsize=(10,5))\n",
    "        axs[0].imshow(np.max(raw_edu, axis=0), cmap='gray')\n",
    "        axs[1].imshow(np.max(seg_edu, axis=0), cmap='jet', interpolation='nearest')\n",
    "        [ax.axis('off') for ax in axs]\n",
    "        [ax.set_title(t) for t,ax in zip(['EdU', 'Replication sites'], axs)]\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        fig, axs = plt.subplots(ncols=2, figsize=(10,5))\n",
    "        axs[0].imshow(np.max(raw_pcna, axis=0), cmap='gray')\n",
    "        axs[1].imshow(np.max(seg_pcna, axis=0), cmap='jet', interpolation='nearest')\n",
    "        [ax.axis('off') for ax in axs]\n",
    "        [ax.set_title(t) for t,ax in zip(['PCNA', 'Replication sites'], axs)]\n",
    "        fig.tight_layout()\n",
    "\n",
    "    \n",
    "    # upscale everything to maximum isotropic resolution\n",
    "    raw = ndi.zoom(raw, zoom_factor)\n",
    "    raw_edu = ndi.zoom(raw_edu, zoom_factor)\n",
    "    raw_pcna = ndi.zoom(raw_pcna, zoom_factor)\n",
    "    seg_dapi = ndi.zoom(seg_dapi, zoom_factor, order=0)\n",
    "    seg_edu = ndi.zoom(seg_edu, zoom_factor, order=0)\n",
    "    seg_pcna = ndi.zoom(seg_pcna, zoom_factor, order=0)\n",
    "    \n",
    "    return raw, raw_edu, raw_pcna, seg_dapi, seg_edu, seg_pcna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files to process (marker files)\n",
    "marker_dir = 'C:/Users/david/Desktop/replication-data/fixed_single_channel2/markers/'\n",
    "marker_files = glob.glob(marker_dir + '*.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on one image with plot\n",
    "#for marker_file in random.sample(marker_files, 1):\n",
    "\n",
    "_ = segment(marker_files[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all multithreaded    \n",
    "with ThreadPoolExecutor() as tpe:\n",
    "    futures = [tpe.submit(segment, marker_file) for marker_file in marker_files]\n",
    "    results = [f.result() for f in tqdm.tqdm(futures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(raw, raw_edu, raw_pcna, seg_dapi, seg_edu, seg_pcna, marker_file):\n",
    "    \n",
    "    # get stage and chase duration from filename\n",
    "    stage = path.Path(marker_file).name.replace('.h5', '').split('_')[1]\n",
    "    chase_dur = float(path.Path(marker_file).name.replace('.h5', '').split('_')[2].replace('h', ''))\n",
    "    fname = path.Path(marker_file).name.replace('.h5', '')\n",
    "    \n",
    "    # we need edt on dapi for dsitance from border\n",
    "    edt = ndi.morphology.distance_transform_edt(seg_dapi)\n",
    "    \n",
    "    # TODO: remove thin objects?\n",
    "    #seg_edu = opening(seg_edu, ball(1))\n",
    "    #seg_pcna = opening(seg_pcna, ball(1))\n",
    "\n",
    "    # get 5% and 99% quantile for normalized intensities\n",
    "    # TODO: make settable?\n",
    "    q_min_dapi, q_max_dapi = np.quantile(raw[seg_dapi], [0.05, 0.99])\n",
    "    q_min_edu, q_max_edu = np.quantile(raw_edu[seg_dapi], [0.05, 0.99])\n",
    "    q_min_pcna, q_max_pcna = np.quantile(raw_pcna[seg_dapi], [0.05, 0.99])\n",
    "\n",
    "    features_edu = defaultdict(list)\n",
    "    centroids_edu = []    \n",
    "    \n",
    "    # go through all connected objects\n",
    "    for rprop in regionprops(seg_edu):\n",
    "        centroids_edu.append(rprop.centroid) # remember centroid -> we need it for nn analysis later\n",
    "\n",
    "        # add all scalar regionprops\n",
    "        for prop in rprop:\n",
    "            try:\n",
    "                if np.isscalar(rprop[prop]):\n",
    "                    features_edu['edu_' + prop].append(rprop[prop])\n",
    "            except NotImplementedError as e:\n",
    "                pass\n",
    "            except QhullError as e: # sometimes happens, add nan in that case, will be resolved later\n",
    "                features_edu['edu_' + prop].append(np.nan)\n",
    "\n",
    "        # add means in distance and intensity images\n",
    "        means = Namespace()\n",
    "        means.edt_mean = np.mean(edt[rprop.slice][rprop.image])    \n",
    "        means.dapi_mean = np.mean(raw[rprop.slice][rprop.image])\n",
    "        means.edu_mean = np.mean(raw_edu[rprop.slice][rprop.image])\n",
    "        means.pcna_mean = np.mean(raw_pcna[rprop.slice][rprop.image])    \n",
    "        means.dapi_mean_norm = (means.dapi_mean - q_min_dapi) / (q_max_dapi - (q_min_dapi))\n",
    "        means.edu_mean_norm = (means.edu_mean - q_min_edu) / (q_max_edu - (q_min_edu))\n",
    "        means.pcna_mean_norm = (means.pcna_mean - q_min_pcna) / (q_max_pcna - (q_min_pcna))\n",
    "        for prop, m in means.__dict__.items():\n",
    "            features_edu['edu_' + prop].append(m)\n",
    "\n",
    "        features_edu['stage'].append(stage)\n",
    "        features_edu['chase_dur'].append(chase_dur)\n",
    "        features_edu['file'].append(fname)\n",
    "\n",
    "    # same as above for pcna\n",
    "    features_pcna = defaultdict(list)\n",
    "    centroids_pcna = []\n",
    "    for rprop in regionprops(seg_pcna):\n",
    "        centroids_pcna.append(rprop.centroid)\n",
    "\n",
    "        # add all scalar regionprops\n",
    "        for prop in rprop:\n",
    "            try:\n",
    "                if np.isscalar(rprop[prop]):\n",
    "                    features_pcna['pcna_' + prop].append(rprop[prop])\n",
    "            except NotImplementedError as e:\n",
    "                pass\n",
    "            except QhullError as e:\n",
    "                features_pcna['pcna_' + prop].append(np.nan)\n",
    "\n",
    "        # add means\n",
    "        means = Namespace()\n",
    "        means.edt_mean = np.mean(edt[rprop.slice][rprop.image])    \n",
    "        means.dapi_mean = np.mean(raw[rprop.slice][rprop.image])\n",
    "        means.edu_mean = np.mean(raw_edu[rprop.slice][rprop.image])\n",
    "        means.pcna_mean = np.mean(raw_pcna[rprop.slice][rprop.image])    \n",
    "        means.dapi_mean_norm = (means.dapi_mean - q_min_dapi) / (q_max_dapi - (q_min_dapi))\n",
    "        means.edu_mean_norm = (means.edu_mean - q_min_edu) / (q_max_edu - (q_min_edu))\n",
    "        means.pcna_mean_norm = (means.pcna_mean - q_min_pcna) / (q_max_pcna - (q_min_pcna))\n",
    "        for prop, m in means.__dict__.items():\n",
    "            features_pcna['pcna_' + prop].append(m)\n",
    "\n",
    "        features_pcna['stage'].append(stage)\n",
    "        features_pcna['chase_dur'].append(chase_dur)\n",
    "        features_pcna['file'].append(fname)\n",
    "\n",
    "\n",
    "    n_edu = len(centroids_edu)\n",
    "    n_pcna = len(centroids_pcna)\n",
    "\n",
    "    # build kdtrees if we have detected objects\n",
    "    if n_edu > 0:\n",
    "        centroids_edu = np.array(centroids_edu)\n",
    "        kd_edu = KDTree(centroids_edu)\n",
    "    if n_pcna > 0:\n",
    "        centroids_pcna = np.array(centroids_pcna)\n",
    "        kd_pcna = KDTree(centroids_pcna)\n",
    "\n",
    "    # which k-nn to use for distances\n",
    "    # TODO: make settable\n",
    "    ks = [1, 3, 5]\n",
    "\n",
    "    # get mean nn-dsitances or nan if not enough neighbours\n",
    "    for c_edu in centroids_edu:\n",
    "        for k in ks:\n",
    "            if n_pcna >= k:\n",
    "                dst, _ = kd_pcna.query(c_edu, k)\n",
    "                features_edu['edu_{}_nn_dist_pcna'.format(k)].append(np.mean(dst))\n",
    "            else:\n",
    "                features_edu['edu_{}_nn_dist_pcna'.format(k)].append(np.nan)\n",
    "        for k in ks:\n",
    "            if n_edu >= (k + 1):\n",
    "                dst, _ = kd_edu.query(c_edu, k+1)\n",
    "                features_edu['edu_{}_nn_dist_edu'.format(k)].append(np.mean(dst[1:]))\n",
    "            else:\n",
    "                features_edu['edu_{}_nn_dist_edu'.format(k)].append(np.nan)\n",
    "\n",
    "    # same for pcna\n",
    "    for c_pcna in centroids_pcna:\n",
    "        for k in ks:\n",
    "            if n_pcna >= (k + 1):\n",
    "                dst, _ = kd_pcna.query(c_pcna, k+1)\n",
    "                features_pcna['pcna_{}_nn_dist_pcna'.format(k)].append(np.mean(dst[1:]))\n",
    "            else:\n",
    "                features_pcna['pcna_{}_nn_dist_pcna'.format(k)].append(np.nan)\n",
    "        for k in ks:\n",
    "            if n_edu >= k:\n",
    "                dst, _ = kd_edu.query(c_pcna, k)\n",
    "                features_pcna['pcna_{}_nn_dist_edu'.format(k)].append(np.mean(dst))\n",
    "            else:\n",
    "                features_pcna['pcna_{}_nn_dist_edu'.format(k)].append(np.nan)\n",
    "\n",
    "                \n",
    "    # remove features for which we added nan in some cases (due to QHullError)\n",
    "    if len(features_edu) > 0:\n",
    "        max_len = np.max([len(v) for v in features_edu.values()])\n",
    "        discard_keys = [k for k,v in features_edu.items() if len(v) != max_len]\n",
    "        for k in discard_keys:\n",
    "            del features_edu[k]\n",
    "\n",
    "    if len(features_pcna) > 0:\n",
    "        max_len = np.max([len(v) for v in features_pcna.values()])\n",
    "        discard_keys = [k for k,v in features_pcna.items() if len(v) != max_len]\n",
    "        for k in discard_keys:\n",
    "            del features_pcna[k]\n",
    "            \n",
    "    # convex image might not be deleted\n",
    "    # TODO: why?\n",
    "    if 'edu_convex_image' in features_edu:\n",
    "        del features_edu['edu_convex_image']\n",
    "    if 'pcna_convex_image' in features_pcna:\n",
    "        del features_pcna['pcna_convex_image']\n",
    "    \n",
    "    return features_edu, features_pcna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate features on single image\n",
    "\n",
    "idx = 3\n",
    "raw, raw_edu, raw_pcna, seg_dapi, seg_edu, seg_pcna = results[idx]\n",
    "marker_file = marker_files[idx]\n",
    "features_edu, features_pcna = get_features(raw, raw_edu, raw_pcna, seg_dapi, seg_edu, seg_pcna, marker_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute features for all images\n",
    "# TODO: multithreading?\n",
    "\n",
    "features = []\n",
    "for i, (result, marker_file) in enumerate(zip(results, marker_files)):\n",
    "    features.append(get_features(*result, marker_file))\n",
    "    print('processed {} of {}: {}'.format(i+1, len(marker_files), marker_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine features for all images\n",
    "def reduce_features(a, b):\n",
    "    for k,v in b[0].items():\n",
    "        a[0][k].extend(b[0][k])\n",
    "    for k,v in b[1].items():\n",
    "        a[1][k].extend(b[1][k])\n",
    "    return a\n",
    "\n",
    "pool_features_edu, pool_features_pcna = reduce(reduce_features, features, (defaultdict(list), defaultdict(list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make DataFrame\n",
    "\n",
    "# calculate median, stddev, count for all features per cell\n",
    "df_agg_edu = pd.DataFrame.from_dict(pool_features_edu).groupby(['file', 'stage', 'chase_dur']).agg(['median', 'count', 'std'])\n",
    "df_agg_edu.columns = df_agg_edu.columns.to_flat_index() # collapse multi-index\n",
    "df_agg_edu.columns = ['_'.join(c) for c in df_agg_edu.columns]\n",
    "df_agg_edu['edu_num_sites'] = df_agg_edu['edu_area_count'] # copy one count column, call it num_sites\n",
    "df_agg_edu = df_agg_edu.drop([c for c in df_agg_edu.columns if c.endswith('count')], 1) # remove all redundant count columns\n",
    "\n",
    "# same for pcna\n",
    "df_agg_pcna = pd.DataFrame.from_dict(pool_features_pcna).groupby(['file', 'stage', 'chase_dur']).agg(['median', 'count', 'std'])\n",
    "df_agg_pcna.columns = df_agg_pcna.columns.to_flat_index()\n",
    "df_agg_pcna.columns = ['_'.join(c) for c in df_agg_pcna.columns]\n",
    "df_agg_pcna['pcna_num_sites'] = df_agg_pcna['pcna_area_count']\n",
    "df_agg_pcna = df_agg_pcna.drop([c for c in df_agg_pcna.columns if c.endswith('count')], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join edu & pcna features\n",
    "df_agg = df_agg_edu.join(df_agg_pcna,  how='outer')\n",
    "\n",
    "# remove columns with only one value, and explicitly euler_number columns (a few sites had euler!=1)\n",
    "df_agg = df_agg.drop([c for c in df_agg.columns if len(df_agg[c].unique()) == 1 or ('euler_number' in c)], 1)\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "df_agg.to_csv('C:/Users/david/Desktop/replication-data/df_agg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
